---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>

I am a third-year Ph.D. student advised by Prof. [Kenji Kawaguchi](https://ml.comp.nus.edu.sg/kawaguchi) at the School of Computing (SoC), National University of Singapore (NUS). Previously, I graduated from Tsinghua University with a B.S. degree in Electronic Engineering. I've had the fortune to work with Prof. [Dongmei Li](http://web.ee.tsinghua.edu.cn/lidongmei/en/index.htm) at Tsinghua University. Afterwards, I joined Center for Speech and Language Technologies (CSLT) as a research intern with Dr. [Lantian Li](http://166.111.134.19:7777/lilt/) and Prof. [Dong Wang](http://wangd.cslt.org/). Then I became an intern in ASR Oteam, Tencent Inc. in Beijing and did research in ASR and Multimodal Learning, organizing [ICPR MSR 2022](https://icprmsr.github.io/index.html) with Dr. [Jian Kang](https://scholar.google.com/citations?hl=zh-CN&user=aNKM4-wAAAAJ), etc.

My research centers on data-centric and generative AI, with a particular emphasis on curating high-quality datasets to enhance the performance and trustworthiness of models such as diffusion and multimodal language models. My previous interest also focuses on audio processing.

---

# üìù Publications

<div class='paper-box'><div class='paper-box-image'><div><img src='images/icpr_track2.png' alt="ICPR MSR 2022" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**ICPR 2022 Challenge on Multi-Modal Subtitle Recognition**

Shan Huang, Shen Huang, Li Lu, Pengfei Hu, Lijuan Wang, Xiang Wang\*, Jian Kang, **Weida Liang**, Lianwen Jin, Yuliang Liu, Yaqiang Wu

*ICPR 2022*

[[Project Page]](https://icprmsr.github.io/)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='images/paper_map.png' alt="Voice Conversion" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Enhanced Exemplar Autoencoder with Cycle Consistency Loss in Any-to-One Voice Conversion**

**Weida Liang**, [Lantian Li](http://166.111.134.19:7777/lilt/), [Dong Wang](http://wangd.cslt.org/), [Wenqiang Du](http://cslt.riit.tsinghua.edu.cn/mediawiki/index.php/Cslt-member-eng#Wenqiang_Du_.28.E6.9D.9C.E6.96.87.E5.BC.BA.29)

*Interspeech 2022*

[[PDF]](https://arxiv.org/pdf/2204.03847.pdf) [[Code]](https://gitlab.com/lwd17/enhanced_examplar_ae/-/tree/main/) [[Project Page]](http://166.111.134.19:7777/liangwd/cycle/)
</div>
</div>

---

# üìÑ Patent

- A cycle loss based voice conversion device, 2022

---

# üéñ Honors and Awards
- **Meritorious Winner** in Mathematical Contest in Modeling, *2019*
- **Bronze Medal** in Chinese Mathematical Olympiad, *2017*

---

# üìñ Education

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/NUS.png" alt="NUS" width="80">
  </div>
  <div>
    <strong>National University of Singapore</strong><br>
    2022.8 - Present<br>
    <em>Ph.D. Student in Computer Science</em><br>
    Advisor: Prof. <a href="https://ml.comp.nus.edu.sg/kawaguchi">Kenji Kawaguchi</a>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/THU.png" alt="Tsinghua University" width="80">
  </div>
  <div>
    <strong>Tsinghua University</strong><br>
    2017.8 - 2021.6<br>
    <em>B.S. in Electronic Engineering</em><br>
    Advisor: Prof. <a href="http://web.ee.tsinghua.edu.cn/lidongmei/en/index.htm">Dongmei Li</a>
  </div>
</div>

---

# üíº Research Experience

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/NUS.png" alt="NUS" width="80">
  </div>
  <div>
    <strong>National University of Singapore</strong> | Ph.D. Student<br>
    <em>2022.8 - Present</em><br>
    Advisor: Prof. <a href="https://ml.comp.nus.edu.sg/kawaguchi">Kenji Kawaguchi</a>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/tencent.jpg" alt="Tencent" width="80">
  </div>
  <div>
    <strong>Tencent</strong> | Research Intern<br>
    <em>2022.3 - 2022.6</em><br>
    Advisor: Dr. <a href="https://scholar.google.com/citations?hl=zh-CN&user=aNKM4-wAAAAJ">Jian Kang</a>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/CSLT.png" alt="CSLT" width="80">
  </div>
  <div>
    <strong>Center for Speech and Language Technologies</strong> | Research Intern<br>
    <em>2021.8 - 2022.4</em><br>
    Advisor: Prof. <a href="http://wangd.cslt.org/">Dong Wang</a>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/THU.png" alt="Tsinghua University" width="80">
  </div>
  <div>
    <strong>Tsinghua University</strong> | B.S. Student<br>
    <em>2017.8 - 2021.6</em><br>
    Advisor: Prof. <a href="http://web.ee.tsinghua.edu.cn/lidongmei/en/index.htm">Dongmei Li</a>
  </div>
</div>

---

# ‚úâÔ∏è Contact

- **Email:** weida_liang[at]u.nus.edu
- **GitHub:** [lwd17](https://github.com/lwd17)
- **Google Scholar:** [Weida Liang](https://scholar.google.com/citations?user=d4DPVXsAAAAJ&hl=zh-CN&oi=ao)
- **WeChat:** [Click to view](images/Wechat.jpeg)
