---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>

I am a fourth-year Ph.D. student advised by Prof. [Kenji Kawaguchi](https://ml.comp.nus.edu.sg/kawaguchi) at the School of Computing (SoC), National University of Singapore (NUS). I am also currently an intern with Prof. [Dawn Song](https://dawnsong.io/)'s team at UC Berkeley. Previously, I graduated from Tsinghua University with a B.S. degree in Electronic Engineering. I've had the fortune to work with Prof. [Dongmei Li](http://web.ee.tsinghua.edu.cn/lidongmei/en/index.htm) at Tsinghua University. Afterwards, I joined Center for Speech and Language Technologies (CSLT) as a research intern with Dr. [Lantian Li](http://166.111.134.19:7777/lilt/) and Prof. [Dong Wang](http://wangd.cslt.org/). Then I became an intern in ASR Oteam, Tencent Inc. in Beijing and did research in ASR and Multimodal Learning, organizing [ICPR MSR 2022](https://icprmsr.github.io/index.html) with Dr. [Jian Kang](https://scholar.google.com/citations?hl=zh-CN&user=aNKM4-wAAAAJ), etc.

My research focuses on large language models, with an emphasis on the security and reliability of LLM-based agents and the foundations of reasoning in modern language models. I study how agentic systems fail under adversarial or ambiguous conditions, and investigate the internal mechanisms that enable or hinder robust multi-step reasoning, with the goal of improving safety, trustworthiness, and generalization.

---

# üî• News

- **[Feb 2026]** We open-sourced a high-quality strategy extraction and reasoning-guidance pipeline, enabling structured reasoning supervision and execution: [strategy-execute-pipeline](https://github.com/lwd17/strategy-execute-pipeline)
- **[Feb 2026]** We released **HM-ReasoningBench**, the first competitive dataset pairing model-generated solutions with human reasoning traces: [HM-ReasoningBench](https://huggingface.co/datasets/jasonlwd/HM-ReasoningBench)
- **[Jan 2026]** Our paper, **CARE: Contextual Adaptation of Recommenders for LLM-based Conversational Recommendation**, was accepted by **ECIR 2026**!

---

# üìù Publications

[1] **CARE: Contextual Adaptation of Recommenders for LLM-based Conversational Recommendation**
Chuang Li, **Weida Liang**, Yang Deng, Hengchang Hu, See-Kiong Ng, Min-Yen Kan, Haizhou Li
*ECIR 2026*
[[PDF]](https://arxiv.org/pdf/2508.13889)

[2] **From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs**
**Weida Liang**, Haonan Wang, Zihang Fu, Nie Zheng, Yifan Zhang, Yao Tong, Tongyao Zhu, Hao Jiang, Chuang Li, Jiaying Wu, Kenji Kawaguchi
*arXiv 2025*
[[PDF]](https://arxiv.org/pdf/2509.23196)

[3] **PromptArmor: Simple yet Effective Prompt Injection Defenses**
Tianneng Shi, Kaijie Zhu, Zhun Wang, Yuqi Jia, Will Cai, **Weida Liang**, Haonan Wang, Hend Alzahrani, etc.
*arXiv 2025*
[[PDF]](https://arxiv.org/pdf/2507.15219)

[4] **ICPR 2022 Challenge on Multi-Modal Subtitle Recognition**
Shan Huang, Shen Huang, Li Lu, Pengfei Hu, Lijuan Wang, Xiang Wang\*, Jian Kang, **Weida Liang**, etc.
*ICPR 2022*
[[Project Page]](https://icprmsr.github.io/)

[5] **Enhanced Exemplar Autoencoder with Cycle Consistency Loss in Any-to-One Voice Conversion**
**Weida Liang**, [Lantian Li](http://166.111.134.19:7777/lilt/), [Dong Wang](http://wangd.cslt.org/), [Wenqiang Du](http://cslt.riit.tsinghua.edu.cn/mediawiki/index.php/Cslt-member-eng#Wenqiang_Du_.28.E6.9D.9C.E6.96.87.E5.BC.BA.29)
*arXiv 2022*
[[PDF]](https://arxiv.org/pdf/2204.03847.pdf) [[Code]](https://gitlab.com/lwd17/enhanced_examplar_ae/-/tree/main/) [[Project Page]](http://166.111.134.19:7777/liangwd/cycle/)

---

# üìñ Education

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/NUS.png" alt="NUS" width="80">
  </div>
  <div>
    <strong>National University of Singapore</strong><br>
    2022.8 - Present<br>
    <em>Ph.D. Student in Computer Science</em><br>
    Advisor: Prof. <a href="https://ml.comp.nus.edu.sg/kawaguchi">Kenji Kawaguchi</a>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/THU.png" alt="Tsinghua University" width="80">
  </div>
  <div>
    <strong>Tsinghua University</strong><br>
    2017.8 - 2021.6<br>
    <em>B.S. in Electronic Engineering</em><br>
    Advisor: Prof. <a href="http://web.ee.tsinghua.edu.cn/lidongmei/en/index.htm">Dongmei Li</a>
  </div>
</div>

---

# üíº Research Experience

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/NUS.png" alt="NUS" width="80">
  </div>
  <div>
    <strong>National University of Singapore</strong> | Ph.D. Student<br>
    <em>2022.8 - Present</em><br>
    Advisor: Prof. <a href="https://ml.comp.nus.edu.sg/kawaguchi">Kenji Kawaguchi</a>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/tencent.jpg" alt="Tencent" width="80">
  </div>
  <div>
    <strong>Tencent</strong> | Research Intern<br>
    <em>2022.3 - 2022.6</em><br>
    Advisor: Dr. <a href="https://scholar.google.com/citations?hl=zh-CN&user=aNKM4-wAAAAJ">Jian Kang</a>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/CSLT.png" alt="CSLT" width="80">
  </div>
  <div>
    <strong>Center for Speech and Language Technologies</strong> | Research Intern<br>
    <em>2021.8 - 2022.4</em><br>
    Advisor: Prof. <a href="http://wangd.cslt.org/">Dong Wang</a>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/THU.png" alt="Tsinghua University" width="80">
  </div>
  <div>
    <strong>Tsinghua University</strong> | B.S. Student<br>
    <em>2017.8 - 2021.6</em><br>
    Advisor: Prof. <a href="http://web.ee.tsinghua.edu.cn/lidongmei/en/index.htm">Dongmei Li</a>
  </div>
</div>

---

# üíª Work Experience

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/ByteDance_logo_English.svg.png" alt="ByteDance" width="80">
  </div>
  <div>
    <strong>ByteDance</strong> | Intern<br>
    <em>2025.7 - Present</em><br>
    TikTok Live
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="margin-right: 20px; min-width: 80px;">
    <img src="images/tencent.jpg" alt="Tencent" width="80">
  </div>
  <div>
    <strong>Tencent</strong> | Intern<br>
    <em>2022.3 - 2022.6</em><br>
    ASR Oteam
  </div>
</div>

---

# ‚úâÔ∏è Contact

- **Email:** weida_liang[at]u.nus.edu
- **GitHub:** [lwd17](https://github.com/lwd17)
- **Google Scholar:** [Weida Liang](https://scholar.google.com/citations?user=d4DPVXsAAAAJ&hl=zh-CN&oi=ao)
- **WeChat:** [Click to view](images/Wechat.jpeg)
